{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "name": "LLMs_can_Schedule_2408_06993.ipynb"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# LLMs can Schedule — arXiv:2408.06993\n### Implementation in Google Colab (T4 GPU)\n\n**Paper:** *LLMs can Schedule* — Abgaryan, Harutyunyan, Cazenave (Université Paris Dauphine - PSL)\n\n**What this notebook does:**\n1. ✅ Install dependencies (OR-Tools, unsloth, trl)\n2. ✅ Generate a JSSP dataset with OR-Tools CP-SAT labels\n3. ✅ Fine-tune **Phi-3-Mini-128K-Instruct** (3.8B) with LoRA via unsloth (fits T4 16 GB)\n4. ✅ Evaluate with the paper's **sampling method** (generate s=10, keep best valid)\n5. ✅ Benchmark against ft06 and la01 with makespan gap reporting\n\n| | Your previous setup | This paper |\n|---|---|---|\n| **Model** | gpt2-medium 345M | Phi-3-Mini 3.8B |\n| **Labels** | EFT heuristic | OR-Tools CP-SAT (provably feasible) |\n| **Format** | compact `J0:M0@0-5` | verbose natural language |\n| **Inference** | single greedy decode | sample s=10, pick best valid |\n| **Validation** | none | regex parse + constraint check |\n\n**Expected result:** ~8–13% gap from optimal on 6×6–10×5 benchmarks (paper reports 8.92% avg gap with s=10).\n\n> **Runtime estimate:** dataset gen ~5 min · training ~90 min · eval ~10 min on T4"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 0 · Check GPU"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import subprocess, sys\n\nresult = subprocess.run([\"nvidia-smi\", \"--query-gpu=name,memory.total\",\n                         \"--format=csv,noheader\"], capture_output=True, text=True)\nif result.returncode == 0:\n    print(\"✓ GPU:\", result.stdout.strip())\nelse:\n    print(\"✗ No GPU found — go to Runtime → Change runtime type → T4 GPU\")\n    sys.exit(1)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1 · Install dependencies\n\n> Takes ~3 minutes on a fresh Colab session."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "%%capture install_log\n!pip install ortools==9.10.4067 unsloth trl transformers datasets accelerate bitsandbytes peft\nprint(\"✓ All packages installed\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2 · Configuration\n\nEdit values here to adjust training size, model, etc."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ── Dataset ─────────────────────────────────────────────────────────────────\nN_SAMPLES       = 3000    # paper uses 120k; 3k is ~5 min and gives decent results\nMIN_SIZE        = 2       # min jobs/machines per dimension\nMAX_SIZE        = 8       # paper goes up to 20; 8 fits T4 context budget\nDUR_MIN         = 5       # min operation duration\nDUR_MAX         = 500     # max operation duration  (paper: 5–500)\nORTOOLS_TIME    = 60      # seconds per problem for OR-Tools (paper: 300)\nDATASET_PATH    = \"jssp_dataset.json\"\n\n# ── Training ─────────────────────────────────────────────────────────────────\nMODEL_NAME      = \"unsloth/Phi-3-mini-128k-instruct\"\nOUTPUT_DIR      = \"/content/jssp_phi3_lora\"\nMAX_SEQ_LEN     = 4096    # paper uses 40k; 4k is T4-safe\nEPOCHS          = 3\nLR              = 2e-4\nLORA_R          = 16\nBATCH_SIZE      = 2\nGRAD_ACCUM      = 8       # effective batch = 2 × 8 = 16\nMAX_TRAIN       = N_SAMPLES\n\n# ── Evaluation ───────────────────────────────────────────────────────────────\nN_INFERENCE_SAMPLES = 10  # paper's sampling parameter s\nTEMPERATURE         = 0.8\nTOP_P               = 0.95\nMAX_NEW_TOKENS      = 1024\n\nprint(\"Config loaded ✓\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3 · Core library\n\nOR-Tools solver, NL formatters, parser, validator — run once."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os, re, json, time\nimport numpy as np\nfrom typing import List, Tuple, Dict, Optional\n\n# ─────────────────────────────────────────────────────────────────────────────\n# 3a. EFT fallback solver (used if OR-Tools times out)\n# ─────────────────────────────────────────────────────────────────────────────\n\ndef _eft_fallback(jobs):\n    n_jobs = len(jobs)\n    n_mach = max(m for job in jobs for m, _ in job) + 1\n    job_free, mach_free = [0]*n_jobs, [0]*n_mach\n    op_idx, start_times, done = [0]*n_jobs, {}, 0\n    total = sum(len(j) for j in jobs)\n    while done < total:\n        best = None\n        for j in range(n_jobs):\n            k = op_idx[j]\n            if k < len(jobs[j]):\n                m, d = jobs[j][k]\n                s = max(job_free[j], mach_free[m])\n                if best is None or s+d < best[0]:\n                    best = (s+d, j, k, m, d, s)\n        _, j, k, m, d, s = best\n        start_times[(j,k)] = s\n        job_free[j] = mach_free[m] = s+d\n        op_idx[j] += 1; done += 1\n    ms = max(start_times[(j,len(job)-1)] + job[-1][1] for j,job in enumerate(jobs))\n    return {\"start_times\": start_times, \"makespan\": ms, \"optimal\": False}\n\n\n# ─────────────────────────────────────────────────────────────────────────────\n# 3b. OR-Tools CP-SAT solver  (paper §4.4)\n# ─────────────────────────────────────────────────────────────────────────────\n\ndef solve_ortools(jobs, time_limit=60):\n    \"\"\"\n    Solve JSSP with Google OR-Tools CP-SAT.\n    Paper config: max_time=300s, num_workers=42, AUTOMATIC_SEARCH.\n    Returns {start_times, makespan, optimal} or falls back to EFT.\n    \"\"\"\n    try:\n        from ortools.sat.python import cp_model\n    except ImportError:\n        return _eft_fallback(jobs)\n\n    model    = cp_model.CpModel()\n    n_jobs   = len(jobs)\n    n_mach   = max(m for job in jobs for m,_ in job) + 1\n    horizon  = sum(d for job in jobs for _,d in job)\n\n    tasks, m2iv = {}, {m: [] for m in range(n_mach)}\n    for j, job in enumerate(jobs):\n        for k, (m, d) in enumerate(job):\n            s  = model.NewIntVar(0, horizon, f\"s_{j}_{k}\")\n            e  = model.NewIntVar(0, horizon, f\"e_{j}_{k}\")\n            iv = model.NewIntervalVar(s, d, e, f\"iv_{j}_{k}\")\n            tasks[(j,k)] = (s, e, m, d)\n            m2iv[m].append(iv)\n\n    for j, job in enumerate(jobs):\n        for k in range(len(job)-1):\n            model.Add(tasks[(j,k+1)][0] >= tasks[(j,k)][1])\n    for m in range(n_mach):\n        model.AddNoOverlap(m2iv[m])\n\n    ms_var = model.NewIntVar(0, horizon, \"ms\")\n    model.AddMaxEquality(ms_var, [tasks[(j,len(job)-1)][1] for j,job in enumerate(jobs)])\n    model.Minimize(ms_var)\n\n    solver = cp_model.CpSolver()\n    solver.parameters.max_time_in_seconds = time_limit\n    solver.parameters.num_search_workers  = min(42, os.cpu_count() or 2)\n    solver.parameters.search_branching    = cp_model.AUTOMATIC_SEARCH\n\n    status = solver.Solve(model)\n    if status not in (cp_model.OPTIMAL, cp_model.FEASIBLE):\n        return _eft_fallback(jobs)\n\n    return {\n        \"start_times\": {(j,k): solver.Value(tasks[(j,k)][0])\n                        for j in range(n_jobs) for k in range(len(jobs[j]))},\n        \"makespan\": solver.Value(ms_var),\n        \"optimal\":  status == cp_model.OPTIMAL,\n    }\n\n\n# ─────────────────────────────────────────────────────────────────────────────\n# 3c. Natural language formatters  (paper Listings 2, 3, 4)\n# ─────────────────────────────────────────────────────────────────────────────\n\n_INSTRS = [\n    \"Optimize schedule for {nj} Jobs across {nm} Machines to minimize makespan. \"\n    \"Each job involves a series of Operations needing specific machines and times. \"\n    \"Operations are processed in order, without interruption, on a single Machine at a time.\",\n    \"Find an optimal schedule for {nj} Jobs on {nm} Machines that minimizes the total \"\n    \"completion time (makespan). Each Job has a fixed sequence of Operations, each \"\n    \"requiring a specific Machine and duration. A Machine can handle only one Job at a time.\",\n    \"Schedule {nj} Jobs across {nm} Machines to minimize makespan. \"\n    \"Jobs must follow their operation order. No machine overlap allowed.\",\n]\n\ndef _instr(nj, nm, rng=None):\n    t = _INSTRS[int(rng.integers(len(_INSTRS))) if rng else 0]\n    return t.format(nj=nj, nm=nm)\n\ndef format_job_centric(jobs, rng=None):\n    \"\"\"Paper Listing 2.\"\"\"\n    nj = len(jobs); nm = max(m for job in jobs for m,_ in job)+1\n    lines = [_instr(nj, nm, rng), \"\", \"Problem:\"]\n    for j, job in enumerate(jobs):\n        lines.append(f\"\\n Job {j} consists of the following Operations:\")\n        for k,(m,d) in enumerate(job):\n            lines.append(f\"  Operation {k} on Machine {m} duration {d} mins.\")\n    return \"\\n\".join(lines)\n\ndef format_machine_centric(jobs, rng=None):\n    \"\"\"Paper Listing 3.\"\"\"\n    nj = len(jobs); nm = max(m for job in jobs for m,_ in job)+1\n    mops = {m: [] for m in range(nm)}\n    for j, job in enumerate(jobs):\n        for k,(m,d) in enumerate(job): mops[m].append((j,k,d))\n    lines = [_instr(nj, nm, rng), \"\", \"Problem:\"]\n    for m in range(nm):\n        lines.append(f\"\\n Machine {m} is used for the following Operations:\")\n        for j,k,d in mops[m]:\n            lines.append(f\"  Job {j} Operation {k} duration {d} mins.\")\n    return \"\\n\".join(lines)\n\ndef format_solution(jobs, result):\n    \"\"\"Paper Listing 4 — sorted by start time.\"\"\"\n    st, ms = result[\"start_times\"], result[\"makespan\"]\n    rows = sorted((st[(j,k)], j, k, m, d)\n                  for j,job in enumerate(jobs)\n                  for k,(m,d) in enumerate(job))\n    lines = [\"Solution:\\n\"]\n    for s,j,k,m,d in rows:\n        lines.append(f\" Job {j} Operation {k} on Machine {m} : {s} + {d} -> {s+d}\")\n    last_k = max(len(job)-1 for job in jobs)\n    lines.append(f\"\\nMakespan: {ms}, as it is the maximum end completion time of Operation {last_k}\")\n    return \"\\n\".join(lines)\n\ndef build_prompt(problem, solution=None):\n    \"\"\"Phi-3 chat template used during training and inference.\"\"\"\n    if solution:\n        return f\"<|user|>\\n{problem.strip()}<|end|>\\n<|assistant|>\\n{solution.strip()}<|end|>\"\n    return f\"<|user|>\\n{problem.strip()}<|end|>\\n<|assistant|>\\n\"\n\n\n# ─────────────────────────────────────────────────────────────────────────────\n# 3d. Output parser + validator  (paper §6.1)\n# ─────────────────────────────────────────────────────────────────────────────\n\n_SOL_RE = re.compile(\n    r\"Job\\s+(\\d+)\\s+Operation\\s+(\\d+)\\s+on\\s+Machine\\s+(\\d+)\"\n    r\"\\s*:\\s*(\\d+)\\s*\\+\\s*(\\d+)\\s*->\\s*(\\d+)\"\n)\n\ndef parse_output(text, jobs):\n    \"\"\"Extract start times from LLM output via regex.\"\"\"\n    expected = {(j,k) for j,job in enumerate(jobs) for k in range(len(job))}\n    st = {}\n    for m in _SOL_RE.finditer(text):\n        j,k,machine,start,dur,end = (int(x) for x in m.groups())\n        if j < len(jobs) and k < len(jobs[j]) and jobs[j][k] == (machine,dur):\n            st[(j,k)] = start\n    return st if set(st.keys()) == expected else None\n\ndef validate(jobs, st):\n    \"\"\"Check precedence + no machine overlap. Returns (valid, makespan).\"\"\"\n    nm = max(m for job in jobs for m,_ in job)+1\n    for j,job in enumerate(jobs):\n        for k in range(1, len(job)):\n            if st[(j,k)] < st[(j,k-1)] + job[k-1][1]:\n                return False, 0\n    for m in range(nm):\n        segs = sorted((st[(j,k)], st[(j,k)]+d)\n                      for j,job in enumerate(jobs)\n                      for k,(mach,d) in enumerate(job) if mach==m)\n        for i in range(1, len(segs)):\n            if segs[i][0] < segs[i-1][1]: return False, 0\n    ms = max(st[(j,len(job)-1)]+job[-1][1] for j,job in enumerate(jobs))\n    return True, ms\n\n\n# ─────────────────────────────────────────────────────────────────────────────\n# 3e. Benchmark instances\n# ─────────────────────────────────────────────────────────────────────────────\n\nFT06 = dict(name=\"ft06\", optimal=55, jobs=[\n    [(2,1),(0,3),(1,6),(3,7),(5,3),(4,6)],\n    [(1,8),(2,5),(4,10),(5,10),(0,10),(3,4)],\n    [(2,5),(3,4),(5,8),(0,9),(1,1),(4,7)],\n    [(1,5),(0,5),(2,5),(3,3),(4,8),(5,9)],\n    [(2,9),(1,3),(4,5),(5,4),(0,3),(3,1)],\n    [(1,3),(3,3),(5,9),(0,10),(4,4),(2,1)],\n])\nLA01 = dict(name=\"la01\", optimal=666, jobs=[\n    [(1,21),(0,53),(4,95),(3,55),(2,34)],\n    [(0,21),(3,52),(4,16),(2,26),(1,71)],\n    [(3,39),(4,98),(1,42),(2,31),(0,12)],\n    [(1,77),(0,55),(4,79),(2,66),(3,77)],\n    [(0,83),(3,34),(2,64),(1,19),(4,37)],\n    [(1,54),(2,43),(4,79),(0,92),(3,62)],\n    [(3,69),(4,77),(1,87),(2,87),(0,93)],\n    [(2,38),(0,60),(1,41),(3,24),(4,83)],\n    [(3,17),(1,49),(4,25),(0,44),(2,98)],\n    [(4,77),(3,79),(2,43),(1,75),(0,96)],\n])\nBENCHMARKS = [FT06, LA01]\n\nprint(\"✓ Core library loaded\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4 · Pipeline sanity check\n\nVerifies OR-Tools → format → parse → validate without a GPU."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"=\" * 55)\nprint(\"Sanity check: OR-Tools → format → parse → validate\")\nprint(\"=\" * 55)\n\njobs = FT06[\"jobs\"]\nt0 = time.time()\nresult = solve_ortools(jobs, time_limit=30)\nelapsed = time.time() - t0\n\nassert result is not None\nstatus = \"OPTIMAL ✓\" if result[\"optimal\"] else \"FEASIBLE (heuristic fallback)\"\nprint(f\"\\nft06 solved in {elapsed:.2f}s  makespan={result['makespan']}  {status}\")\nprint(f\"Known optimal = 55  |  gap = {(result['makespan']-55)/55*100:.1f}%\")\n\nproblem_nl  = format_job_centric(jobs)\nsolution_nl = format_solution(jobs, result)\n\nprint(\"\\nProblem excerpt:\")\nfor line in problem_nl.split(\"\\n\")[3:7]:\n    print(\" \", line)\n\nprint(\"\\nSolution excerpt:\")\nfor line in solution_nl.split(\"\\n\")[2:6]:\n    print(\" \", line)\n\nst = parse_output(solution_nl, jobs)\nassert st is not None, \"Parse failed!\"\nok, ms = validate(jobs, st)\nassert ok, \"Validation failed!\"\nprint(f\"\\nParse + validate ✓  makespan={ms}\")\n\nprompt = build_prompt(problem_nl, solution_nl)\nprint(f\"\\nTraining prompt length: {len(prompt)} chars / ~{len(prompt)//4} tokens\")\nprint(\"\\n✓ All checks passed — ready to generate dataset\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 5 · Generate training dataset\n\nOR-Tools solves each random JSSP instance and stores the feasible schedule as the training label.\nThis is what the paper does for their 120k dataset — we default to 3k for a fast run."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from tqdm.auto import tqdm\n\ndef generate_dataset(n_samples=N_SAMPLES, out_path=DATASET_PATH,\n                     min_size=MIN_SIZE, max_size=MAX_SIZE,\n                     dur_min=DUR_MIN, dur_max=DUR_MAX,\n                     ortools_time=ORTOOLS_TIME, seed=42):\n\n    rng = np.random.default_rng(seed)\n    data, skipped = [], 0\n\n    pbar = tqdm(total=n_samples, desc=\"Generating\")\n    while len(data) < n_samples:\n        nj = int(rng.integers(min_size, max_size+1))\n        nm = int(rng.integers(min_size, max_size+1))\n\n        jobs = [[(int(m), int(d))\n                 for m, d in zip(rng.permutation(nm).tolist(),\n                                 rng.integers(dur_min, dur_max+1, size=nm).tolist())]\n                for _ in range(nj)]\n\n        result = solve_ortools(jobs, time_limit=ortools_time)\n        if result is None:\n            skipped += 1\n            continue\n\n        use_job = bool(rng.integers(2))\n        problem  = format_job_centric(jobs, rng) if use_job else format_machine_centric(jobs, rng)\n        solution = format_solution(jobs, result)\n\n        data.append({\n            \"text\":         build_prompt(problem, solution),\n            \"makespan\":     result[\"makespan\"],\n            \"optimal\":      result[\"optimal\"],\n            \"num_jobs\":     nj,\n            \"num_machines\": nm,\n        })\n        pbar.update(1)\n\n    pbar.close()\n    with open(out_path, \"w\") as f:\n        json.dump(data, f)\n\n    opt_pct  = 100 * sum(d[\"optimal\"] for d in data) / len(data)\n    avg_ms   = np.mean([d[\"makespan\"] for d in data])\n    avg_size = np.mean([d[\"num_jobs\"]*d[\"num_machines\"] for d in data])\n    print(f\"\\n✓ Saved {len(data)} samples → {out_path}\")\n    print(f\"  Optimal: {opt_pct:.1f}%  |  Avg makespan: {avg_ms:.0f}  |  Avg problem size: {avg_size:.1f} ops\")\n    print(f\"  Skipped: {skipped} (solver timeout/infeasible)\")\n    return data\n\ndataset = generate_dataset()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 6 · Fine-tune Phi-3-Mini with LoRA\n\nLoads `Phi-3-Mini-128K-Instruct` in 4-bit via unsloth, attaches LoRA adapters, trains.\n\n**T4 memory budget:** ~13 GB used with batch=2, grad_accum=8, seq_len=4096."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from unsloth import FastLanguageModel\nfrom datasets import Dataset\nfrom trl import SFTTrainer\nfrom transformers import TrainingArguments\nimport torch\n\n# ── Load base model ──────────────────────────────────────────────────────────\nprint(\"Loading Phi-3-Mini-128K-Instruct (4-bit) …\")\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name     = MODEL_NAME,\n    max_seq_length = MAX_SEQ_LEN,\n    dtype          = None,        # auto-detect bf16 / fp16\n    load_in_4bit   = True,\n)\nprint(f\"✓ Base model loaded  |  {sum(p.numel() for p in model.parameters())/1e9:.2f}B params\")\n\n# ── Attach LoRA adapters ─────────────────────────────────────────────────────\nmodel = FastLanguageModel.get_peft_model(\n    model,\n    r              = LORA_R,\n    target_modules = [\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\n                      \"gate_proj\",\"up_proj\",\"down_proj\"],\n    lora_alpha     = LORA_R,\n    lora_dropout   = 0.05,\n    bias           = \"none\",\n    use_gradient_checkpointing = \"unsloth\",\n    random_state   = 42,\n)\nmodel.print_trainable_parameters()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ── Prepare dataset ──────────────────────────────────────────────────────────\nwith open(DATASET_PATH) as f:\n    raw = json.load(f)\nraw = raw[:MAX_TRAIN]\n\nnp.random.default_rng(0).shuffle(raw)\nhf_dataset = Dataset.from_list(raw)\nprint(f\"Training on {len(hf_dataset)} samples  (max_seq_len={MAX_SEQ_LEN})\")\n\n# ── Train ────────────────────────────────────────────────────────────────────\ntrainer = SFTTrainer(\n    model              = model,\n    tokenizer          = tokenizer,\n    train_dataset      = hf_dataset,\n    dataset_text_field = \"text\",\n    max_seq_length     = MAX_SEQ_LEN,\n    dataset_num_proc   = 2,\n    args = TrainingArguments(\n        per_device_train_batch_size = BATCH_SIZE,\n        gradient_accumulation_steps = GRAD_ACCUM,\n        warmup_steps                = 50,\n        num_train_epochs            = EPOCHS,\n        learning_rate               = LR,\n        bf16                        = True,\n        fp16                        = False,\n        logging_steps               = 20,\n        optim                       = \"adamw_8bit\",\n        weight_decay                = 0.01,\n        lr_scheduler_type           = \"cosine\",\n        output_dir                  = OUTPUT_DIR,\n        save_steps                  = 500,\n        report_to                   = \"none\",\n    ),\n)\n\nprint(\"\\nStarting training …\")\ntrainer_stats = trainer.train()\nprint(f\"\\n✓ Training complete\")\nprint(f\"  Total steps: {trainer_stats.global_step}\")\nprint(f\"  Final loss:  {trainer_stats.training_loss:.4f}\")\nprint(f\"  Time:        {trainer_stats.metrics['train_runtime']/60:.1f} min\")\n\nmodel.save_pretrained(OUTPUT_DIR)\ntokenizer.save_pretrained(OUTPUT_DIR)\nprint(f\"\\n✓ Model saved → {OUTPUT_DIR}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 7 · Sampling-based inference\n\nThe paper's key contribution: generate **s** candidates, validate each, return the best valid makespan.\nThis is what pushes the LLM from ~20% gap (greedy) down to ~8.92% (s=10)."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "FastLanguageModel.for_inference(model)  # enable unsloth's fast kernel\n\ndef solve_with_llm(jobs, n_samples=N_INFERENCE_SAMPLES,\n                   temperature=TEMPERATURE, top_p=TOP_P,\n                   max_new_tokens=MAX_NEW_TOKENS, fmt=\"job\"):\n    \"\"\"\n    Paper sampling method (§6.2):\n      1. Generate s candidate solutions with temperature sampling.\n      2. Parse + validate each with regex + constraint checker.\n      3. Return the one with the lowest valid makespan.\n    \"\"\"\n    problem  = format_job_centric(jobs) if fmt == \"job\" else format_machine_centric(jobs)\n    prompt   = build_prompt(problem)\n    inputs   = tokenizer(prompt, return_tensors=\"pt\",\n                         truncation=True, max_length=2048).to(\"cuda\")\n\n    best_ms, best_result, valid_count = None, None, 0\n\n    for i in range(n_samples):\n        with torch.no_grad():\n            out = model.generate(\n                **inputs,\n                max_new_tokens = max_new_tokens,\n                do_sample      = True,\n                temperature    = temperature,\n                top_p          = top_p,\n                pad_token_id   = tokenizer.eos_token_id,\n            )\n        text = tokenizer.decode(out[0][inputs[\"input_ids\"].shape[1]:],\n                                skip_special_tokens=True)\n        st = parse_output(text, jobs)\n        if st is None:\n            continue\n        ok, ms = validate(jobs, st)\n        if ok:\n            valid_count += 1\n            if best_ms is None or ms < best_ms:\n                best_ms, best_result = ms, {\"makespan\": ms, \"start_times\": st, \"text\": text}\n\n    return best_result, valid_count\n\nprint(\"✓ Inference function ready\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 8 · Benchmark evaluation\n\nEvaluate on **ft06** (6×6, optimal=55) and **la01** (10×5, optimal=666) — same benchmarks as the paper."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "all_results = []\n\nfor bench in BENCHMARKS:\n    name, optimal, jobs = bench[\"name\"], bench[\"optimal\"], bench[\"jobs\"]\n    nj = len(jobs); nm = max(m for job in jobs for m,_ in job)+1\n    print(f\"\\n{'='*55}\")\n    print(f\"  {name.upper()}  ({nj}×{nm})   optimal makespan = {optimal}\")\n    print(f\"{'='*55}\")\n\n    t0 = time.time()\n    result, valid_count = solve_with_llm(jobs, n_samples=N_INFERENCE_SAMPLES)\n    elapsed = time.time() - t0\n\n    if result:\n        ms  = result[\"makespan\"]\n        gap = (ms - optimal) / optimal * 100\n        print(f\"  Best makespan : {ms}\")\n        print(f\"  Optimality gap: {gap:.2f}%\")\n        print(f\"  Valid / Total : {valid_count} / {N_INFERENCE_SAMPLES}\")\n        print(f\"  Time          : {elapsed:.1f}s\")\n        print(f\"\\n  Schedule (sorted by start time):\")\n        for line in result[\"text\"].split(\"\\n\")[1:8]:\n            print(f\"    {line}\")\n    else:\n        ms, gap = None, None\n        print(f\"  No valid solution found in {N_INFERENCE_SAMPLES} samples\")\n        print(f\"  Time: {elapsed:.1f}s\")\n\n    all_results.append(dict(instance=name, optimal=optimal,\n                            makespan=ms, gap_pct=gap,\n                            valid_solutions=valid_count,\n                            n_samples=N_INFERENCE_SAMPLES,\n                            elapsed=elapsed))\n\n# Summary\nprint(f\"\\n{'='*55}\")\nprint(\"RESULTS SUMMARY\")\nprint(f\"{'='*55}\")\nvalid = [r for r in all_results if r[\"gap_pct\"] is not None]\nif valid:\n    avg_gap = np.mean([r[\"gap_pct\"] for r in valid])\n    print(f\"\\n  Average gap from optimal : {avg_gap:.2f}%\")\n    print(f\"  Paper reports            : ~8.92%  (Phi-3, s=10, 10×10 test set)\")\nfor r in all_results:\n    g = f\"{r['gap_pct']:.2f}%\" if r[\"gap_pct\"] is not None else \"no solution\"\n    print(f\"  {r['instance']:8s}  makespan={r['makespan']}  gap={g}  \"\n          f\"valid={r['valid_solutions']}/{r['n_samples']}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 9 · Save results & download model"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import json\n\n# Save benchmark results\nwith open(\"/content/jssp_results.json\", \"w\") as f:\n    json.dump(all_results, f, indent=2)\nprint(\"✓ Results saved → /content/jssp_results.json\")\n\n# Download helper\nfrom google.colab import files\n\nprint(\"\\nDownload options:\")\nprint(\"  Run the next cell to download the results JSON.\")\nprint(\"  The LoRA adapter is in:\", OUTPUT_DIR)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Optional: download results\n# from google.colab import files\n# files.download(\"/content/jssp_results.json\")\n\n# Optional: zip and download the LoRA adapter weights\n# import shutil\n# shutil.make_archive(\"/content/jssp_phi3_lora\", \"zip\", OUTPUT_DIR)\n# files.download(\"/content/jssp_phi3_lora.zip\")\nprint(\"Uncomment lines above to download files\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 10 · Quick interactive test (optional)\n\nTry the model on a custom problem."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Define your own JSSP instance here\n# Format: jobs[j] = [(machine_id, duration), ...]  — each job visits all machines in order\n\ncustom_jobs = [\n    [(0, 3), (1, 2), (2, 5)],   # Job 0: M0(3h) → M1(2h) → M2(5h)\n    [(1, 4), (0, 6), (2, 2)],   # Job 1: M1(4h) → M0(6h) → M2(2h)\n    [(2, 3), (1, 5), (0, 4)],   # Job 2: M2(3h) → M1(5h) → M0(4h)\n]\n\nprint(\"Problem:\")\nprint(format_job_centric(custom_jobs))\n\nprint(\"\\nSolving with LLM (s=5 samples) …\")\nresult, valid_count = solve_with_llm(custom_jobs, n_samples=5)\n\nif result:\n    print(f\"\\n✓ Valid schedule found  makespan={result['makespan']}  ({valid_count}/5 valid candidates)\")\n    print(\"\\nSchedule:\")\n    for line in result[\"text\"].split(\"\\n\")[:10]:\n        print(\" \", line)\n\n    # Compare with OR-Tools optimal\n    ref = solve_ortools(custom_jobs, time_limit=10)\n    if ref:\n        gap = (result[\"makespan\"] - ref[\"makespan\"]) / ref[\"makespan\"] * 100\n        print(f\"\\nOR-Tools optimal: {ref['makespan']}  |  LLM gap: {gap:.1f}%\")\nelse:\n    print(\"No valid solution found — try increasing n_samples or check the model trained correctly\")"
  }
 ]
}